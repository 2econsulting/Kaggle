output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=20, max_model=nrow(params))
Sys.time() - start
# title : homecredit_w_XGB
# authro : jacob
# library
options(scipen = 999)
rm(list=ls())
gc(reset=TRUE)
library(parallel)
library(data.table)
library(e1071)
library(caret)
library(Metrics)
library(xgboost)
# tuning code
path_code = "~/GitHub/2econsulting/Kaggle/R/Classification/XGB"
source(file.path(path_code,"tuneXGB.R"))
source(file.path(path_code,"cvpredictXGB.R"))
# set path
path_input = "~/Kaggle/homecredit/input"
path_output = "~/Kaggle/homecredit/output"
# ..
y = "TARGET"
ml = "XGB"
max_model = 2
sample_rate = 0.001
kfolds = 2
# read data
data = fread(file.path(path_input, 'will/will_train.csv'))
test = fread(file.path(path_input, 'will/will_test.csv'))
submit = fread(file.path(path_input, 'will/will_test.csv'))
# sampling
set.seed(1)
sample_num =round(nrow(data)*sample_rate)
# ..
data$SK_ID_CURR <- NULL
test$SK_ID_CURR <- NULL
# ..
data[is.na(data)] <- -9999
test[is.na(test)] <- -9999
# ------------------------
#  optimal Depth Range
# ------------------------
params <- expand.grid(
max_depth =  c(2, 3, 4, 5, 6, 7, 8, 9)
)
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=20, max_model=nrow(params))
Sys.time()
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = 1,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = -1,
n_jobs = 1,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = -1,
n_jobs = -1,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = -1,
n_jobs = -1,
a=2,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = -1,
n_jobs = 8,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = 1,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
# title : tuneXGB
# author : jacob
tuneXGB <- function(data, y, params, k, max_model=NULL){
if(k<2) stop(">> k is very small \n")
require(Matrix)
require(xgboost)
data <- as.data.frame(data)
# shuffle params
if(ncol(params)==1){
params = params
cat(">> cartesian grid search, params not shuffled! \n")
Sys.sleep(3)
}else{
set.seed(1)
params = params[sample(nrow(params)),]
cat(">> random grid search, params shuffled! \n")
Sys.sleep(3)
}
# convert char to factor
if(sum(sapply(data, function(x) is.character(x)))>0){
char_idx <- which(sapply(data, function(x) is.character(x)))
data[char_idx] <- lapply(data[char_idx], as.factor)
}
data_y = data[,y]
data_x = data[,which(colnames(data)!=y)]
# need for xgboost
colnames(data_x) <- paste0(rep("X",ncol(data_x)),1:ncol(data_x))
sparse_matrix_train <- sparse.model.matrix(~.-1, data = data_x)
ddata <- xgb.DMatrix(data = sparse_matrix_train, label = data_y)
if(is.numeric(max_model)){
max_model <- max_model
}else{
max_model <- nrow(params)
}
if(nrow(params)<max_model) stop(">> max_model is lower than nrow(params) \n")
output <- list()
for(i in 1:max_model){
set.seed(1)
ml_xgb <- xgb.cv(
data = ddata,
eval_metric  = "logloss",
maximize = FALSE,
objective = "binary:logistic",
nrounds = 1000,
early_stopping_rounds = 50,
# nthread = detectCores(logical=F), # not working
nthread = 1,
tree_method = "hist",
grow_policy = "lossguide",
nfold = k,
stratified = TRUE,
params = as.list(sapply(as.list(params),"[",i)),
prediction = TRUE # cvpredict
)
output$scores[i] <- min(ml_xgb$evaluation_log$test_logloss_mean)
output$models[i] <- list(ml_xgb)
}
output$scores <- data.frame(head(params, max_model), logloss=output$scores)
output$scores <- output$scores[order(output$scores$logloss, decreasing = F), ]
return(output)
}
params <- expand.grid(
max_depth =  c(2, 3, 4, 5, 6, 7, 8, 9)
)
start = Sys.time()
optimalDepthRange <- tuneXGB(head(data, sample_num), y=y, params=params, k=kfolds, max_model=nrow(params))
Sys.time() - start
