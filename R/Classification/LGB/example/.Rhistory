setwd("~/GitHub/2econsulting/Kaggle/R/Classification/XGB/example")
# title : homecredit_w_XGB
# authro : jacob
# library
setwd("~/GitHub/2econsulting/Kaggle/R/Classification/XGB/example/")
options(scipen = 999)
rm(list=ls())
gc(reset=TRUE)
library(rAutoFE)
library(data.table)
library(e1071)
library(caret)
library(Metrics)
library(xgboost)
source('../tuneXGB.R')
source('../cvpredictXGB.R')
# read data
data = fread('./input/homecredit_data.csv')
test = fread('./input/homecredit_test.csv')
sample = fread('./input/sample_submission.csv')
# ..
data$SK_ID_CURR <- NULL
test$SK_ID_CURR <- NULL
# ..
data[is.na(data)]<- 0
test[is.na(test)]<- 0
# ------------------------
#  optimal Depth Range
# ------------------------
params <- expand.grid(
max_depth = c(2,3,4)
)
optimalDepthRange <- tuneXGB(data, y="TARGET", params=params, cv=5, max_model=nrow(params))
optimalDepthRange$scores
# ------------------------
# optimal hyper-params
# ------------------------
params <- expand.grid(
max_depth = head(optimalDepthRange$scores$max_depth, 3),
eta = seq(0.01, 1, 0.3),
gamma = seq(0, 1, 0.2),
subsample = seq(0.6, 1, 0.9),
colsample_bytree = seq(0.5, 1, 0.1),
min_child_weight = seq(1, 40, 1),
max_delta_step = seq(1, 10, 1)
)
optimalParams <- tuneXGB(data, y="TARGET", params=params, cv=5, max_model=3)
optimalParams$scores
# ------------------------
# cvpredict catboost
# ------------------------
source('../cvpredictXGB.R')
params = as.list(head(optimalParams$scores[names(params)],1))
output <- cvpredictXGB(data, test, k=5, y="TARGET", params=params)
output$crossvalidation_score
output$cvpredict_score
# title : homecredit_w_CatBoost
# author : jacob
# library
setwd("~/GitHub/2econsulting/Kaggle/R/Classification/CatBoost/example/")
options(scipen = 999)
rm(list=ls())
gc(reset=TRUE)
library(caret)
library(rAutoFE)
library(data.table)
library(e1071)
library(Metrics)
library(catboost)
source("../tuneCatBoost.R")
source("../cvpredictCatBoost.R")
# read data
data = fread('./input/homecredit_data.csv')
test = fread('./input/homecredit_test.csv')
sample = fread('./input/sample_submission.csv')
# ..
data$SK_ID_CURR <- NULL
test$SK_ID_CURR <- NULL
names <- which(sapply(data, class) != "numeric")
data[, (names) := lapply(.SD, as.numeric), .SDcols = names]
# missing value
data[is.na(data)] <- 0
test[is.na(test)] <- 0
# ------------------------
#  optimal Depth Range
# ------------------------
source("../tuneCatBoost.R")
params <- expand.grid(
depth = c(3,4,5,6,7),
learning_rate = 0.05, # 0.05
iterations = 10,
border_count = 32,
rsm = 1,
l2_leaf_reg = 3
)
optimalDepthRange <- tuneCatBoost(data, y="TARGET", max_model=nrow(params), cv=3, grid=params)
optimalDepthRange$results
# ------------------------
# optimal hyper-params
# ------------------------
params <- expand.grid(
depth = head(optimalDepthRange$results$depth,3),
learning_rate = c(0.1, 0.03),
l2_leaf_reg = c(0, 3 ,1, 2),
rsm = c(1,0.9,0.8),
border_count = 32,
iterations = 10
)
optimalParams <- tuneCatBoost(data, y="TARGET", max_model=5, cv=3, grid=params)
optimalParams$results
# ------------------------
# cvpredict catboost
# ------------------------
source("../cvpredictCatBoost.R")
params <- as.list(optimalParams$bestTune)
output = cvpredictCatBoost(data, test, k=3, y="TARGET", params=params)
output$cvpredict_score
output$crossvalidation_score
# title : homecredit_w_lightgbm
# author : jacob
# library
setwd("~/GitHub/2econsulting/Kaggle/R/Classification/LGB/example")
options(scipen = 999)
rm(list=ls())
gc(reset=TRUE)
library(rAutoFE)
library(data.table)
library(e1071)
library(caret)
library(Metrics)
library(lightgbm)
source("../tuneLGB.R")
source("../cvpredictLGB.R")
# read data
data = fread('./input/homecredit_data.csv')
test = fread('./input/homecredit_test.csv')
sample = fread('./input/sample_submission.csv')
# ..
data$SK_ID_CURR <- NULL
test$SK_ID_CURR <- NULL
names <- which(sapply(data, class) != "numeric")
data[, (names) := lapply(.SD, as.numeric), .SDcols = names]
# ..
data[is.na(data)]<- 0
test[is.na(test)]<- 0
# ------------------------
#  optimal Depth Range
# ------------------------
params <- expand.grid(
max_depth = c(2,3,4)
)
optimalDepthRange <- tuneLGB(data, y="TARGET", params=params, cv=5, max_model=nrow(params))
optimalDepthRange$scores
# ------------------------
# optimal hyper-params
# ------------------------
params <- expand.grid(
max_depth = head(optimalDepthRange$scores$max_depth, 3),
learning_rate = seq(0.01, 1, 0.3),
subsample = seq(0.6, 1, 0.9),
colsample_bytree = seq(0.5, 1, 0.1),
min_child_weight = seq(1, 40, 1),
max_delta_step = seq(1, 10, 1)
)
optimalParams <- tuneLGB(data, y="TARGET", params=params, cv=5, max_model=10)
optimalParams$scores
# ------------------------
# cvpredict catboost
# ------------------------
params = as.list(head(optimalParams$scores[names(params)],1))
output <- cvpredictLGB(data, test, k=10, y="TARGET", params=params)
output$crossvalidation_score
output$cvpredict_score
# title : homecredit_w_lightgbm
# author : jacob
# library
setwd("~/GitHub/2econsulting/Kaggle/R/Classification/LGB/example")
options(scipen = 999)
rm(list=ls())
gc(reset=TRUE)
library(rAutoFE)
library(data.table)
library(e1071)
library(caret)
library(Metrics)
library(lightgbm)
source("../tuneLGB.R")
source("../cvpredictLGB.R")
# read data
data = fread('./input/homecredit_data.csv')
test = fread('./input/homecredit_test.csv')
sample = fread('./input/sample_submission.csv')
# ..
data$SK_ID_CURR <- NULL
test$SK_ID_CURR <- NULL
names <- which(sapply(data, class) != "numeric")
data[, (names) := lapply(.SD, as.numeric), .SDcols = names]
# ..
data[is.na(data)]<- 0
test[is.na(test)]<- 0
# ------------------------
#  optimal Depth Range
# ------------------------
params <- expand.grid(
max_depth = c(2,3,4)
)
optimalDepthRange <- tuneLGB(data, y="TARGET", params=params, cv=5, max_model=nrow(params))
optimalDepthRange$scores
# ------------------------
# optimal hyper-params
# ------------------------
params <- expand.grid(
max_depth = head(optimalDepthRange$scores$max_depth, 3),
learning_rate = seq(0.01, 1, 0.3),
subsample = seq(0.6, 1, 0.9),
colsample_bytree = seq(0.5, 1, 0.1),
min_child_weight = seq(1, 40, 1),
max_delta_step = seq(1, 10, 1)
)
optimalParams <- tuneLGB(data, y="TARGET", params=params, cv=5, max_model=10)
optimalParams$scores
# ------------------------
# cvpredict catboost
# ------------------------
params = as.list(head(optimalParams$scores[names(params)],1))
output <- cvpredictLGB(data, test, k=10, y="TARGET", params=params)
output$crossvalidation_score
output$cvpredict_score
